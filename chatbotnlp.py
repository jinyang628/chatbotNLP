# -*- coding: utf-8 -*-
"""chatBotNLP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qKFn5em7L4IUKLo0f8Nvaj-u90TuZuWr
"""

!pip install transformers

from torch.utils.data import Dataset
import json
from transformers import GPT2LMHeadModel, GPT2Tokenizer
from torch.optim import Adam
from torch.utils.data import DataLoader
import tqdm
import torch

class ChatData(Dataset):
    def __init__(self, path:str, tokenizer):
        self.data = json.load(open(path, "r"))

        self.text = []
        for i in self.data:
            for j in i['dialog']:
                self.text.append(j['text'])

        for idx, i in enumerate(self.text):
            try:
                self.text[idx] = "<startofstring> "+i+" <bot>: "+self.text[idx+1]+" <endofstring>"
            except:
                break

        self.text = self.text[:5000]
        self.text_encoded = tokenizer(self.text,max_length=40, truncation=True, padding="max_length", return_tensors="pt")
        self.input_ids = self.text_encoded['input_ids']
        self.attention_mask = self.text_encoded['attention_mask']

    def __len__(self):
        return len(self.text)

    def __getitem__(self, idx):
        return (self.input_ids[idx], self.attention_mask[idx])

def train(chatData, model, optim):

    epochs = 2

    # Iterate over the specific number of epochs
    for i in tqdm.tqdm(range(epochs)):
        for text, attnMask in chatData:
            text = text.to(device)
            attnMask = attnMask.to(device)
            # The optimizer's gradient is reset to zero 
            optim.zero_grad()
            """
            After the model processes the input and generates the output, the 
            .loss attribute is accessed to obtain the computed loss value. The 
            loss value represents the discrepancy between the predicted output 
            and the target labels
            """
            loss = model(text, attention_mask=attnMask, labels=text).loss
            # The gradients are computed for all trainable parameters 
            loss.backward()
            # Optimizer updates the model's parameters based on the computed gradients 
            optim.step()
        # After each epoch, the current state of the model is saved to a file
        torch.save(model.state_dict(), "model_state.pt")

def infer(inp):
    inp = "<startofstring> "+inp+" <bot>: "
    inp = tokenizer(inp, return_tensors="pt")
    text = inp["input_ids"].to(device)
    attnMask = inp["attention_mask"].to(device)
    output = model.generate(text, attention_mask=attnMask )
    output = tokenizer.decode(output[0])
    return output


device = "cuda" if torch.cuda.is_available() else "mps" if torch.backends.mps.is_available() else "cpu"

tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
tokenizer.add_special_tokens({"pad_token": "<pad>", 
                                "bos_token": "<startofstring>",
                                "eos_token": "<endofstring>"})
tokenizer.add_tokens(["<bot>:"])

model = GPT2LMHeadModel.from_pretrained("gpt2")
model.resize_token_embeddings(len(tokenizer))

model = model.to(device)

chatData = ChatData("./chat_data.json", tokenizer)
chatData =  DataLoader(chatData, batch_size=64)

model.train()

optim = Adam(model.parameters(), lr=1e-3)

# Train the model
train(chatData, model, optim)

# Run the model
print("infer from model : ")
while True:
  inp = input()
  print(infer(inp))